{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Carregamento de bibliotecas e configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 02:34:26.833198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 02:34:26.983865: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 02:34:27.694495: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/bring/miniconda3/envs/wsl2_machine_learning_test_env/lib/\n",
      "2024-09-26 02:34:27.695608: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/bring/miniconda3/envs/wsl2_machine_learning_test_env/lib/\n",
      "2024-09-26 02:34:27.695617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dispositivo: NVIDIA GeForce RTX 3080 Ti\n",
      "Memória total: 12287 MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "# Marcar como True, caso deseje efetuar o treinamento a partir de um modelo que ja tenha passado pelo processo de fine-tuning \n",
    "# OBS: Modelo deve estar presente no diretório \"./trained_model\"\n",
    "load_pretrained = True\n",
    "\n",
    "# Treina o modelo apenas com uma parcela dos registros da tabela original.\n",
    "# Ex: n = -1 = treinar com todos os itens da tabela\n",
    "n_samples = -1\n",
    "\n",
    "# Verifica se há GPUs disponíveis\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory // (1024 ** 2) # MB\n",
    "    torch.cuda.empty_cache()  # Limpa cache da GPU para garantir um melhor funcionamento do modelo\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    total_memory = \"-\"\n",
    "\n",
    "print(f\"\\nTreinando no dispositivo: {device_name}\")\n",
    "print(f\"Memória total: {total_memory} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bring/miniconda3/envs/wsl2_machine_learning_test_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega modelo e tokenizer (BERT para Question Answering)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Armazena modelo/tokenizer original para comparação posterior\n",
    "model_raw = model \n",
    "tokenizer_raw = tokenizer\n",
    "\n",
    "# Carrega modelo com fine-tuning anterior ja realizado\n",
    "if load_pretrained:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./trained_model\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"./trained_model\")\n",
    "\n",
    "# Move o modelo para o dispositivo (GPU ou CPU)\n",
    "model.to(device)\n",
    "model_raw.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando dados de treino...\n",
      "\n",
      "Preparando dados de treino...\n",
      "\n",
      "1390403 registros válidos encontrados...\n",
      "\n",
      "\n",
      "1390403 registros selecionados para treinamento...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Girls Ballet Tutu Neon Pink</td>\n",
       "      <td>High quality 3 layer ballet tutu. 12 inches in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mog's Kittens</td>\n",
       "      <td>Judith Kerr&amp;#8217;s best&amp;#8211;selling adventu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Girls Ballet Tutu Neon Blue</td>\n",
       "      <td>Dance tutu for girls ages 2-8 years. Perfect f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Prophet</td>\n",
       "      <td>In a distant, timeless place, a mysterious pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rightly Dividing the Word</td>\n",
       "      <td>--This text refers to thePaperbackedition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248608</th>\n",
       "      <td>[180 Days Warranty] ZeroLemon&amp;reg; Samsung Gal...</td>\n",
       "      <td>Features:This is the World's highest capacity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248611</th>\n",
       "      <td>Teenage Mutant Ninja Turtles Donatello Wizard ...</td>\n",
       "      <td>With a pointed bed sheet hat, robe decorated w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248612</th>\n",
       "      <td>~Shave Ready~ Shaving Straight Razor 6/8&amp;quot;...</td>\n",
       "      <td>Inside this amazing set is a 6/8\" round point ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248617</th>\n",
       "      <td>Cont  Removable Paper Label</td>\n",
       "      <td>Continuous Length Removable Paper Label 2-3/7\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248618</th>\n",
       "      <td>1/2&amp;quot;  Round Paper Labels</td>\n",
       "      <td>0.5\" - Diameter - 1200 Label(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "0                              Girls Ballet Tutu Neon Pink   \n",
       "3                                            Mog's Kittens   \n",
       "7                              Girls Ballet Tutu Neon Blue   \n",
       "12                                             The Prophet   \n",
       "13                               Rightly Dividing the Word   \n",
       "...                                                    ...   \n",
       "2248608  [180 Days Warranty] ZeroLemon&reg; Samsung Gal...   \n",
       "2248611  Teenage Mutant Ninja Turtles Donatello Wizard ...   \n",
       "2248612  ~Shave Ready~ Shaving Straight Razor 6/8&quot;...   \n",
       "2248617                        Cont  Removable Paper Label   \n",
       "2248618                      1/2&quot;  Round Paper Labels   \n",
       "\n",
       "                                                   content  \n",
       "0        High quality 3 layer ballet tutu. 12 inches in...  \n",
       "3        Judith Kerr&#8217;s best&#8211;selling adventu...  \n",
       "7        Dance tutu for girls ages 2-8 years. Perfect f...  \n",
       "12       In a distant, timeless place, a mysterious pro...  \n",
       "13              --This text refers to thePaperbackedition.  \n",
       "...                                                    ...  \n",
       "2248608  Features:This is the World's highest capacity ...  \n",
       "2248611  With a pointed bed sheet hat, robe decorated w...  \n",
       "2248612  Inside this amazing set is a 6/8\" round point ...  \n",
       "2248617  Continuous Length Removable Paper Label 2-3/7\"...  \n",
       "2248618                    0.5\" - Diameter - 1200 Label(s)  \n",
       "\n",
       "[1390403 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar dataset\n",
    "def load_dataset(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            data = [json.loads(line) for line in file]\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(f\"Erro: O arquivo {path} não foi encontrado.\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise Exception(\"Erro: O arquivo contém dados inválidos.\")\n",
    "    \n",
    "\n",
    "print(\"\\nCarregando dados de treino...\")\n",
    "raw_data = load_dataset(\"trn.json\")\n",
    "\n",
    "print(\"\\nPreparando dados de treino...\")\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "df_raw = df_raw[['title', 'content']].replace('', np.nan).dropna() # Selecionando as colunas necessárias (Removendo a coluna 'answer', e todas as linhas com valores vazios e nulos)\n",
    "print(f\"\\n{len(df_raw)} registros válidos encontrados...\\n\")\n",
    "\n",
    "if n_samples > 0:\n",
    "    df = df_raw.sample(n=n_samples, random_state=42)\n",
    "else:\n",
    "    df = df_raw\n",
    "\n",
    "print(f\"\\n{len(df)} registros selecionados para treinamento...\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1ea4212df5484f8b02b46c32d5b137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1390403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transforma dados em um Dataset da Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Função para encontrar os índices de início e fim da resposta no contexto\n",
    "def add_token_positions(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Adiciona as posições de início e fim das respostas no contexto e tokeniza os dados.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Dicionário com os exemplos de entrada (títulos e conteúdos).\n",
    "        tokenizer: Tokenizer do modelo BERT.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dados tokenizados com posições de início e fim das respostas.\n",
    "    \"\"\"\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Considerando que a resposta será o conteúdo inteiro\n",
    "    for i in range(len(examples['content'])):\n",
    "        context = examples['content'][i]\n",
    "        start_idx = 0\n",
    "        end_idx = len(context) - 1  # A resposta será todo o contexto\n",
    "\n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "\n",
    "    # Tokenizar o conteúdo\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['content'],\n",
    "        examples['title'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"start_positions\"] = start_positions\n",
    "    tokenized_inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokeniza o dataset e adiciona as posições de início e fim das respostas\n",
    "tokenized_data = dataset.map(lambda x: add_token_positions(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bring/miniconda3/envs/wsl2_machine_learning_test_env/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417123' max='417123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [417123/417123 23:30:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.278700</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.031500</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.821300</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34761' max='34761' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34761/34761 34:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação do modelo: {'eval_loss': nan, 'eval_runtime': 2082.4421, 'eval_samples_per_second': 133.536, 'eval_steps_per_second': 16.692, 'epoch': 3.0}\n",
      "Modelo treinado e tokenizer salvos em './trained_model'.\n"
     ]
    }
   ],
   "source": [
    "# Função de Fine-tuning com BERT\n",
    "def fine_tune_model(tokenized_datasets, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Realiza o fine-tuning do modelo BERT utilizando os dados tokenizados.\n",
    "\n",
    "    Args:\n",
    "        tokenized_datasets (Dataset): Dataset tokenizado para treinamento e validação.\n",
    "        tokenizer: Tokenizer do modelo BERT.\n",
    "        model: Modelo BERT para fine-tuning.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: Objeto Trainer após o treinamento.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['test'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer  # Retorna o objeto trainer para avaliação posterior\n",
    "   \n",
    "\n",
    "# Remove as colunas que não são usadas pelo modelo\n",
    "tokenized_data = tokenized_data.remove_columns(['content', 'title'])\n",
    "\n",
    "# Separa dados em treino e teste\n",
    "tokenized_data = tokenized_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# Fine-tune no modelo\n",
    "trainer = fine_tune_model(tokenized_data, tokenizer, model)\n",
    "\n",
    "# Avalia o modelo após o fine-tuning\n",
    "eval_results = trainer.evaluate(tokenized_data['test'])\n",
    "print(f\"Avaliação do modelo: {eval_results}\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n",
    "print(\"Modelo treinado e tokenizer salvos em './trained_model'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validando eficiêcia do modelo pré e pós fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, questions, contexts):\n",
    "    \"\"\"\n",
    "    Gera respostas para perguntas dadas com base nos contextos fornecidos, removendo tokens especiais.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo BERT treinado.\n",
    "        tokenizer: Tokenizer do modelo BERT.\n",
    "        questions (list): Lista de perguntas.\n",
    "        contexts (list): Lista de contextos correspondentes.\n",
    "\n",
    "    Returns:\n",
    "        str: Resposta gerada pelo modelo sem os tokens especiais.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(contexts, questions, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Pegando os índices da resposta\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores)\n",
    "\n",
    "    # Converter tokens de volta para string\n",
    "    answer_tokens = all_tokens[answer_start:answer_end + 1]\n",
    "\n",
    "    # Remover tokens especiais como [CLS], [SEP] e [PAD]\n",
    "    answer_tokens = [token for token in answer_tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "\n",
    "    # Juntar os tokens restantes para formar a resposta final\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respostas geradas pelo modelo pré e pós fine-tuning:\n",
      "\n",
      "Pergunta: I Don't Care - Learning About Respect (Values)\n",
      "Resposta: (Pré fine-tuning): \n",
      "Resposta: (Pós fine-tuning): brian moses lives in the small sussex village of crowhurst with his wife anne, a loopy labrador called honey and a collection of bad - tempered chickens. he first worked as a teacher but has now been a professional children ' s poet since 1988. to date he has over 200 books published including volumes of his own poetry such as holding the hands of angels ( salt ) and behind the staffroom door ( macmillan ), anthologies such as the secret lives of teachers and aliens stole my underpants ( both macmillan ), picture books such as beetle in the bathroom and trouble at the dinosaur cafe ( both puffin ) and non - fiction titles such as titanic : lost & saved ( wayland ). over 1 million copies of brian ' s poetry books have now been sold by macmillan and in 2005 he was nominated for both the clpe award and the spoken word award. brian also visits schools to run writing workshops and perform his own poetry and percussion shows. to date he has visited well over 2500 schools and libraries throughout the uk. he has made several appearances at the edinburgh festival, been writer in residence at castle cornet on guernsey, on the romney, hythe and dymchurch light railway and at raf schools in cyprus. recently he has visited several international schools in germany, switzerland, belgium, italy, spain, france and ireland. he has performed his poetry at borders on second avenue, new york and in september 2006 he was invited to iceland to take part in ' kids in the marsh ' - a festival of children ' s poetry and song. at the request of prince charles he spoke at the prince ' s summer school for teachers in 2007 at cambridge university. he is one of ten children ' s poets invited by then poet laureate andrew motion to feature on the national poetry archive. favourite book : ' turtle diary ' by russell hoban. favourite movie : one flew over the cuckoo ' s nest favourite music : bob dylan i don ' t care - learning about respect ( values )\n",
      "\n",
      "\n",
      "Pergunta: Ty Beanie Ballz Zips - Bee\n",
      "Resposta: (Pré fine-tuning): 5 \" long in diameter and are made of ty ' s best selling fabric - ty silk. one of ty ' s newest collections. look for the familiar heart - shaped tag that means you & # x2019 ; ve purchased an authentic ty product. handmade with the finest quality standards in the industry. collect them all. high quality for a low price. ty beanie ballz zips - bee\n",
      "Resposta: (Pós fine-tuning): beanie ballz are a little wild and whacky, toss ' em and they always land on their feet. they are 5 \" long in diameter and are made of ty ' s best selling fabric - ty silk. one of ty ' s newest collections. look for the familiar heart - shaped tag that means you & # x2019 ; ve purchased an authentic ty product. handmade with the finest quality standards in the industry. collect them all. high quality for a low price. ty beanie ballz zips - bee\n",
      "\n",
      "\n",
      "Pergunta: 12' Hanging Lantern Cord with On/off Switch Whirled Planet Brand\n",
      "Resposta: (Pré fine-tuning): \n",
      "Resposta: (Pós fine-tuning): basic 12 ' electric cord for hanging lanterns. these have an on / off switch and a standard polarized plug for us and canadian electric systems. please note : other companies claim to be selling the same product listed here. their product may look the same, but we cannot guarantee the quality and safety of their products. as the manufacturer of this product, we use only the highest quality materials and test each piece. if you are buying this product and the seller is listed outside the us, it is a counterfeit product. buyer beware! available in white. 12 ' hanging lantern cord with on / off switch whirled planet brand\n",
      "\n",
      "\n",
      "Pergunta: Carrot Chips ~ 8 Oz. Bag\n",
      "Resposta: (Pré fine-tuning): dried carrot chips carrot chips ~ 8 oz. bag\n",
      "Resposta: (Pós fine-tuning): freeze dried carrot chips carrot chips ~ 8 oz. bag\n",
      "\n",
      "\n",
      "Pergunta: Thirsties Diaper Pail Liner\n",
      "Resposta: (Pré fine-tuning): \n",
      "Resposta: (Pós fine-tuning): thirsties diaper pail liner is made from coated nylon with drawstring closure to keep messes and moisture sealed within. designed to fit a standard 52 - quart garbage pail available at your local department store. to clean simply toss the duffel in your washer with your diapers. the coated nylon allows your diapers to slip out into your washer with ease so your hands stay clean at laundry time. we recommend purchasing at least two of these to have in rotation. thirsties diaper pail liner\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seleciona algumas perguntas aleatoriamente para validação visual\n",
    "df_samples = df.sample(n=5, random_state=42)\n",
    "test_questions = df_samples[\"title\"].sort_values().to_list()\n",
    "test_contexts = df_samples[\"content\"].sort_values().to_list()\n",
    "\n",
    "# Testar o modelo antes do fine-tuning\n",
    "print(\"\\nRespostas geradas pelo modelo pré e pós fine-tuning:\\n\")\n",
    "for index, row in df_samples.iterrows():\n",
    "    response = generate_responses(model, tokenizer, row[\"title\"], row[\"content\"])\n",
    "    response_raw = generate_responses(model_raw, tokenizer_raw, row[\"title\"], row[\"content\"])\n",
    "    \n",
    "    print(f\"Pergunta: {row['title']}\")\n",
    "    print(f\"Resposta: (Pré fine-tuning): {response_raw}\")\n",
    "    print(f\"Resposta: (Pós fine-tuning): {response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground - Utilize esse trecho para interagir com o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     print(\"Insira uma pertunta e uma pergunta e um contexto ao modelo:\")\n",
    "#     question = input(\"Pergunta: \")\n",
    "#     context = input(\"Contexto: \")\n",
    "#     if question == \"\" or context == \"\":\n",
    "#         break\n",
    "#     else:\n",
    "#         response = generate_responses(model, tokenizer, question, context)\n",
    "#         print(f\"Pergunta: {question}\")\n",
    "#         print(f\"Resposta: {response}\")\n",
    "#         print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl2_machine_learning_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
